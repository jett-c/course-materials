\chapter{Algorithms for Solving Non-Linear Equations}

\lesson[2]{03/09/2025}{Bisection Method}

\begin{nameless}
    Solve the roots (zeros) of a function. Given a function \(f: \R \to \R\), find root(s) \(x^\ast\) satisfying
    \[f(x^\ast) = 0.\]
\end{nameless}

\section{Bisection Method}

Recall the Intermediate Value Theorem (IVT):
\begin{theorem}[Intermediate value theorem]
    Let \(f \in \mathcal{C}^0[a, b]\) and \(f(a)f(b) < 0\), then there exists \(p \in (a,b)\) satisfying \(f(p) = 0\).
\end{theorem}

This motivates the bisection method to find the root approximately.

\begin{algorithm}[H]
    \KwData{bounds \(a, b \in \R, f\in \mathcal{C}^0[a, b]\) satisfying \(f(a)f(b)<0\), error \(\varepsilon\)}
    \KwResult{approximate root \(p\)}
    \(a_1 \leftarrow a\) \; \(b_1 \leftarrow b\) \;
    \(i \leftarrow 1\) \;
    \(p_1 = \frac{a_1+b_1}{2}\) \;
    \While{not condition}{
        \If{\(f(a_i)f(p_i) > 0\)}{
            \(a_{i+1} \leftarrow p_i\) \; \(b_{i+1} \leftarrow b_i\) \;
        }
        \Else{
            \(a_{i+1} \leftarrow a_i\) \; \(b_{i+1} \leftarrow p_i\) \;
        }
        \(i++\) \;
        \(p_i = \frac{a_i+b_i}{2}\) \;        
    }
    \(p \leftarrow p_i\) \;
    \caption{Bisection method}
\end{algorithm}

The condition can be
\begin{itemize}
    \item \(|p_i - p_{i-1}|<\varepsilon\): absolute change of iteration,
    \item \(\frac{p_i - p_{i-1}}{p_i} < \varepsilon\): relative change of iteration,
    \item \(|f(p_i)| < \varepsilon\): magnitude, or
    \item \(i > i_\text{max}\): maximum iteration.
\end{itemize}
Note that in other root-finding algorithms, we also employ one of these conditions.

\begin{theorem}[Convergence speed of bisection method]
    Assume the bisection method condition holds and such root \(p\) is unique. The bisection method generates a sequence \(p_n \to p\) with
    \[|p_n - p| \leq \frac{b-a}{2^n}\]
\end{theorem}
\begin{proof}
    Note that \(p \in [a_n, b_n], p_n = \frac{a_n+b_n}{2}\) and \(b_n-a_n = \frac{b-a}{2^{n-1}}\).
\end{proof}

\begin{remark}
    In other word, \(p_n = p + O(2^{-n})\). Also, this bound is only the worst case, while the actual error is usually much smaller.
\end{remark}

This also means if we want to achieve \(|p_n-p| < \varepsilon\), we need
\[n > \log_2 \frac{\varepsilon}{b-a}.\]

The convergence rate is
\[r = \inf\{r: \limsup_{k \to \infty}\frac{|p_{k+1} - p|}{|p_k - p|} < +\infty\}.\]
For bisection method, the convergence rate is linear since
\(|p-p_{n+1}| \leq \frac{|p-p_n|}{2}\). This means the convergence is independent of the original range.

Advantage: Always converge.

Disadvantage: Does not work for higher dimension, modification required.

\lesson[3]{10/09/2025}{Fixed Point Iteration}

\section{Fixed Point Iteration}

\begin{definition}[Fixed point]
    For a function \(g: X \to X\), a point \(p \in X\) is said to be a fixed point if \(p = g(p)\).
\end{definition}

Thus a root finding problem can be converted to a fixed point problem.

\begin{theorem}[Existence of a fixed point]
    Let \(g \in \mathcal{C}^0: [a, b] \to [a, b]\), then there exists a fixed point in \([a, b]\).
\end{theorem}
\begin{proof}
    Consider \(h(x) = g(x)-x\). If \(h(a) = 0\) \text{or} \(h(b) = 0\) then we are done. Else, apply the intermediate value theorem.
\end{proof}

The uniqueness can be shown by the Banach fixed point theorem.

\begin{theorem}[Banach fixed point theorem]
    Let \((X, d)\) be a non-empty metric space and \(f: X \to X\) be a contraction mapping i.e. there exists \(0 \leq L < 1\) such that \(|f(x) - f(y)| < L|x-y|\) for any \(x, y \in X\), then there exists a unique fixed point for \(f\).
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item[]
        \item Show that \(f\) is continuous
        \item For any \(x_0 \in x\), define \(x_n = f(x_{n-1})\). Show that \((x_n)\) is a Cauchy sequence, thus converges.
        \item Show that fixed point is unique by considering \(|x^\ast - \bar{x}^\ast|\).
    \end{enumerate}
\end{proof}

Since an interval of \(\R\) is a metric space, this gives the fixed point method.

\begin{algorithm}[H]
    \KwData{bounds \(a, b \in \R\), \(f: \in \mathcal{C}^0[a, b]\), initial guess \(p_0 \in [a, b]\)}
    \KwResult{approximate root \(p\)}
    Rewrite \(f(x) = 0 \To x = g(x), g \in \mathcal{C}^0[a, b]\) \;
    \(i \leftarrow 0\) \;
    \While{not condition}{
        \(p_i \leftarrow g(p_i)\) \;
        \(i ++\) \;
    }
    \(p \leftarrow p_{i}\) \;
    \caption{Fixed point method}
    \label{alg:fixed-point-method}
\end{algorithm}

The condition can be
\begin{itemize}
    \item \(|p_i - p_{i-1}| < \varepsilon\)
    \item \(i > i_\text{max}\).
\end{itemize}

In implementation, we would like to choose \(g\) to be a contraction mapping, then the error bound is
\[|p_n - p| \leq L^n |p_0-p| \To |p_n-p| =O(L^n).\]

A value of \(L\) can be easily found if \(g\) is differentiable.

\begin{theorem}[Mean value theorem]
    Let \(g\) be a function continuous on \([a, b]\) and differentiable on \((a, b)\), then there exists \(c \in (a, b)\) such that
    \[g'(c) = \frac{g(b) - g(a)}{b-a}.\]
\end{theorem}

In this case, \(L = \sup_{x \in [a, b]} |g'(x)|\).

Also, note that the choice of \(g\) is crucial to whether, and how fast the algorithm converges.

\begin{example}
    To solve \(f(x) = x^3 + 4x^2 - 10 = 0\), we have these choices for fixed point problem \(x=\)
    \begin{itemize}
        \item \(g_1(x) = x-x^3-4x^2+10\);
        \item \(g_2(x) = \sqrt{\frac{10}{x}-4x}\);
        \item \(g_3(x) = \frac{\sqrt{10-x^3}}{2}\);
        \item \(g_4(x) = \sqrt{\frac{10}{4+x}}\);
        \item \(g_5(x) = x - \frac{x^3+4x^2-10}{3x^2+8x}\).
    \end{itemize}
    These choices give different results
    \begin{itemize}
        \item \(g_1\): diverges;
        \item \(g_2\): diverges and goes out of domain;
        \item \(g_3\): 10 s.f. precision in 30 iterations;
        \item \(g_4\): 10 s.f. in 15 iterations;
        \item \(g_5\): 10 s.f. in 4 iterations.
    \end{itemize}
\end{example}

Advantage: very fast if contraction map chosen correctly, easily implemented on a calculator

Disadvantage: choosing a wrong contraction map leads to divergence

\lesson[4]{15/09/25}{Newton Method}

\section{Newton Method}

Consider \(f \in \mathcal{C}^2[a, b]\) with a root \(f(p)= 0\), Taylor expansion gives
\[f(p) =f(p_0) + (p-p_0) f'(p_0) + \frac{(p-p_0)^2}{2}f''(\xi),\]
where \(\xi\) is between \(p, p_0\). Hence, we get
\[p = p_0 - \frac{f(p_0)}{f'(p_0)}-\frac{(p-p_0)^2}{2}\frac{f''(\xi)}{f'(p_0)} \approx p_0 - \frac{f(p_0)}{f'(p_0)}.\]

This gives the Newton's method.

\begin{algorithm}[H]
    \KwData{bounds \(a, b \in \R\), \(f\) differentiable in \((a, b)\), initial guess \(p_0 \in [a, b]\)}
    \KwResult{approximate root \(p\)}
    \(i \leftarrow 0\) \;
    \While{not condition}{
        \If{\(f'(p_i) = 0\)}{
            Raise division-by-zero exception. \;          
        }
        \Else{
            \(p_{i+1} = p_i - \frac{f(p_i)}{f'(p_i)}\) \;
        }
        \(i ++\) \;
    }
    \(p \leftarrow p_{i}\) \;
    \caption{Newton's method}
    \label{alg:newton-method}
\end{algorithm}

Geometrically, this gives the intersection between the tangent line at \(p_i\) and the \(x\)-axis.

\begin{theorem}[Quadratic convergence rate]
    Assume \(f\in\mathcal{C}^2[a, b]\) satisfies \(f(p) = 0\) and \(f'(p) \neq 0\), then for a closed neighborhood of \(p\), the Newton's method converges in quadratic rate. In other words, there exists \(\delta > 0\), such that for any \(p_0 \in [p_0-\delta, p_0+\delta]\), the sequence
    \[p_n = p_{n-1} - \frac{f(p_{n-1})}{f'(p_{n-1})}\]
    converges to \(p\) and
    \[\limsup_{n \to \infty}\left|\frac{p-p_n}{(p-p_{n-1})^2}\right|<+\infty.\]
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item[]
        \item Shows that the Newton's method is equivalent to a fixed point iteration
        \item Show that there is a contraction mapping.
        \item Taylor expansion around \(p_n\) to estimate error.
    \end{enumerate}
\end{proof}

To prevent overshooting, we may use adaptive step size instead
\[p_{i+1} = p_i - \lambda_i \frac{f(p_i)}{f'(p_i)}.\]
This slows down the error decay rate but relaxes the requirement for initial guess.

\lesson[5]{17/09/25}{Secant Method and comparison of different methods}

\section{Secant Method}

Motivation: When computation of derivative is unfeasible, approximate derivative by
\[f'(p_n) \approx \frac{f(p_n) - f(p_{n-1})}{p_n - p_{n-1}},\]
as both terms are same in the limit \(n \to \infty\).

\begin{algorithm}[H]
    \KwData{function \(f\), initial guesses \(p_0, p_1\)}
    \KwResult{approximate root \(p\)}
    \(i \leftarrow 1\) \;
    \While{not condition}{
        \(p_{i+1} = p_i - f(p_i) \frac{p_i-p_{i-1}}{f(p_i) - f(p_{i-1})}\) \;
        \(i ++\) \;
    }
    \caption{Secant method}
\end{algorithm}

\begin{theorem}
    If \(f\in\mathcal{C}^2[a, b]\) and there exists \(p \in (a, b)\) such that \(f(p) = 0\) and \(f'(p) \neq 0\), then within an open neighborhood of \(p\), the sequence generated by the secant method converges to \(p\) and convergence rate is \(\phi = \frac{1+\sqrt{5}}{2}\).
\end{theorem}

Advantages: superlinear convergence, no need for derivatives

Disadvantages: slower than Newton's method, requires good initial guesses

The method of false position combines bisection method and secant method for both robustness and speed. Bisection is stable but slow (linear), secant is fast (superlinear) but unstable.

\begin{algorithm}[H]
    \KwData{initial guesses \(p_0, p_1\), function \(f \in \mathcal{C}^0[p_0, p_1]\) with \(f(p_0)f(p_1)<0\)}
    \KwResult{approximate root \(p\)}
    \(i \leftarrow 1\) \;
    \(a_1 \leftarrow p_0\) \;
    \(b_1 \leftarrow p_1\)
    \While{not condition}{
        \(p_{i+1} = b_i - f(b_i) \frac{b_i-a_i}{f(b_i) - f(a_i)}\) \;
        \If{\(f(p_{i+1})f(b_i)<0\)}{
            \(b_{i+1} \leftarrow b_i\) \;
            \(a_{i+1} \leftarrow p_{i+1}\) \;
        }
        \Else{
            \(b_{i+1} \leftarrow p_{i+1}\) \;
            \(a_{i+1} \leftarrow a_i\) \;
        }
        \(i ++\;\)
    }
    \caption{Method of False Position}
\end{algorithm}

i.e. Now we find the secant from points with different signs in function values.

\lesson[6]{24/09/25}{Extensions to High Dimensional Nonlinear Systems and Optimization}

\section{High Dimensional Generalization}

\subsection{Fixed Point Iteration}

\begin{definition}[Normed vector space]
    \((V, \|\cdot\|)\) is said to be a normed vector space if \(V\) is a vector space and \(\|\cdot\|: V \to \R\) is the norm function such that \(\forall \mathbf{x}, \mathbf{y} \in V, c \in \R\)
    \begin{itemize}
        \item (Positive definite) \(\|\mathbf{x}\| \geq 0, = 0 \iff \mathbf{x} = \mathbf{0}\),
        \item (Absolute homogeneity) \(\|c\mathbf{x}\| = |c| \|\mathbf{x}\|\),
        \item (Triangle inequality \(\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|\).
    \end{itemize}
\end{definition}

\begin{example}
    For the Euclidean space \(\mathbb{R}^d\), the 2-norm is
    \[\|\mathbf{x}\|_2 = \sqrt{x_1^2 + \cdots +x_d^2}.\]
\end{example}

While the 2-norm is commonly used in \(\R^d\) due to rotational invariance, other norms share the same topological property.

\begin{prop}
    All norms in \(\R^d\) are equivalent i.e. for any two norms \(\|\cdot\|, \|\cdot \|'\), there exists \(C > 0\) such that for any \(\mathbf{x} \in \R^d\)
    \[\frac{\|\mathbf{x}\|}{C} \leq \|\mathbf{x}\|' \leq C\|\mathbf{x}\|.\]
\end{prop}

Also, note that
\begin{prop}
    Normed vector space is a metric space with distance function \(d(\mathbf{x}, \mathbf{y}):=\|\mathbf{x} - \mathbf{y}\|\).
\end{prop}

Hence, we can apply the Banach fixed point theorem to guarantee a fixed point. The algorithm is same as Algorithm \ref{alg:fixed-point-method}, but replacing the interval with a subset of \(\R^d\).

Note that in fixed point iteration, we only used information for \(\mathbf{p}_{i}\) to build \(\mathbf{p}_{i+1}\). If extra \(m\) entries of history information i.e. \(\mathbf{p}_n, \cdots, \mathbf{p}_{n-m}\), we can minimize the residual by finding a convex combination.

\begin{algorithm}[H]
    \KwData{subset \(D \subset \R^d\), function \(\mathbf{F}: D \to D\), initial guess \(\mathbf{p}_0 \in D\), number of historic entries \(m \in \N\)}
    \KwResult{approximate root \(\mathbf{p}\)}
    Rewrite \(\mathbf{F}(\mathbf{x}) = \mathbf{0} \iff \mathbf{x} = \mathbf{G}(\mathbf{x})\) \;
    \(i \leftarrow 0\) \;
    \(\mathbf{r}_0 = \mathbf{G}(\mathbf{p}_0) - \mathbf{p}_0\) \;
    \While{not condition}{
        \(m_i \leftarrow \min(m, i)\) \;
        \(\hat{\boldsymbol{\alpha}} \leftarrow \mathrm{argmin}_{\boldsymbol{\alpha}} \|\alpha_0 \mathbf{r}_0 + \cdots + \alpha_{m_i}\mathbf{r}_{m_i}\|_2: \sum_{k=0}^{m_i}\alpha_k = 1\) \;
        \(\mathbf{p}_{i+1} \leftarrow \sum_{k=0}^{m_i}\hat{\alpha_k}\mathbf{G}(p_{i-m_i+k})\) \;
        \(\mathbf{r}_{i+1} = \mathbf{G}(\mathbf{p}_{i+1}) - \mathbf{p}_{i+1}\) \;
        \(i ++\) \;
    }
    \caption{Anderson acceleration}
    \label{alg:anderson-acceleration}
\end{algorithm}

This algorithm has a superlinear convergence compared to the linear convergence 

\subsection{Newton Method}

\begin{definition}[Jacobian matrix]
    The \(d \times d\) Jacobian matrix \(\mathbf{J}(\mathbf{x})\) of a function \(\mathbf{f}(\mathbf{x}): \mathbb{R}^d \to \mathbb{R}^d\) is defined by
    \[\mathbf{J} = \begin{pmatrix}
        \frac{\partial F_1}{\partial x_1} & \cdots & \frac{\partial F_1}{\partial x_d} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial F_d}{\partial x_1} & \cdots & \frac{\partial F_d}{\partial x_d}
    \end{pmatrix}\]
    i.e. \(J_{ij} = \frac{\partial J_i}{\partial x_j}.\)
\end{definition}

The Jacobian matrix generalizes the derivative of a single-variable function. The algorithm is similar to Algorithm \ref{alg:newton-method}, while replacing the derivative with \(\mathbf{J}\). 

Similar with the 1D case, the convergence speed is quadratic, and a good initial guess is required. However, the computation of \(\mathbf{J}^{-1}\) can be expensive. This motivates the quasi-Newton method to approximate \(\mathbf{J}^{-1}_i\) by cheap computations (JFNK), which also benefit from the super-linear convergence speed.

\section{Optimization}

Aim: find \(\mathbf{p} \in \R^d\) minimizing \(f(\mathbf{x})\).

If \(f\) is differentiable, then at the local minima, \(\nabla f(\mathbf{x} ) = 0\)

\begin{algorithm}[H]
    \KwData{Differentiable \(f: \R^d \to \R\), learning rate \(\alpha > 0\), initial point \(\mathbf{x}_0 \in \R^d\)}
    \KwResult{approximate argmin \(\mathbf{x}: \nabla f(\mathbf{x}) \approx \mathbf{0}\)}
    \(i \leftarrow 0\) \;
    \While{not condition}{
        \(\mathbf{x}_{i+1} = \mathbf{x}_i - \alpha \nabla f(\mathbf{x}_k)\) \;
        \(i ++ \)\;
    }
    \(\mathbf{x} \leftarrow \mathbf{x}_i\)\;
    \caption{Gradient descent}
\end{algorithm}

The condition is \(i > i_\text{max}\) or \(\|\nabla f(\mathbf{x}_i)\| \leq \varepsilon\).

The problem can also be seen as solving the fixed point problem \(\mathbf{x} = \mathbf{x} - \alpha \nabla f(\mathbf{x})\).

Advantage: intuitive, simple, guaranteed to obtain a solution for sufficiently small \(\alpha\)

Disadvantage: overshooting \(\alpha\) may lead to unstable solution, maxima or saddle point.

Alternatively, we can use the Newton method. Before so, we would like to generalize the second derivative by the Hessian matrix.
\begin{definition}[Hessian matrix]
    The Hessian matrix \(\mathbf{H}(\mathbf{x})\) of a function \(f(\mathbf{x}): \R^d \to \R\) is
    \[\mathbf{H} = \nabla^2 f = \begin{pmatrix}
        \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_d} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial^2 f}{\partial x_dx_1} & \cdots & \frac{\partial^2 f}{\partial x_d^2}
    \end{pmatrix}.\]
\end{definition}

\begin{algorithm}[H]
    \KwData{subset \(D \subset \R^d\), \(f(\mathbf{x}): \R^d\to\R \in \mathcal{C}^2\), initial guess \(\mathbf{x}_0\)}
    \KwResult{approximate minimizer \(\mathbf{x}: \nabla f(\mathbf{x}) \approx \mathbf{0}\)}
    \(i \leftarrow 0\) \;
    \While{not condition}{
        \(\mathbf{x}_{i+1} = \mathbf{x}_i - \mathbf{H}_i^{-1}\nabla f(\mathbf{x}_i)\) \;
        \(i ++\) \;
    }
    \caption{Newton method for optimization}
\end{algorithm}

Although the convergence is quadratic, computing the Hessian matrix can be expensive.
